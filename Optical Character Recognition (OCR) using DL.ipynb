{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7f57b-db3d-4a00-874d-b0540ab2513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default libraries, packages for data management, visualization and Computer vision libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Sklearn package -> function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tensorflow packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, MaxPool2D, \n",
    "                                     BatchNormalization, Flatten, GlobalAveragePooling2D, Input)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.applications import EfficientNetB7, MobileNetV2, VGG19, DenseNet121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb4f06-25e2-4a0c-8f57-e0a30ccef7aa",
   "metadata": {},
   "source": [
    "เตรียมฟังก์ชันที่จำเป็นต้องใช้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62d12a-1cb4-45c4-bc8d-fd09b78de36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_df(path : str):\n",
    "    \"\"\"\n",
    "    This function to retrieve all images from targeted folder in a file, the\n",
    "    folder must be divided hirarchally in which each class contains its images individually.\n",
    "    ________________________________________________________________________________________________\n",
    "    Arguments-\n",
    "    \n",
    "    path: String -> the main folder directory that contains train/test folders\n",
    "    \n",
    "    ________________________________________________________________________________________________\n",
    "    Return-\n",
    "    \n",
    "    DataFrame: contains the images path and label corresponding to every image\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyz'    # to include lowercase letters only\n",
    "    for cls in os.listdir(path):\n",
    "        cls_path = os.path.join(path,cls)\n",
    "        cls_name = cls.split('_')[0]\n",
    "        if not cls_name in chars:\n",
    "            continue\n",
    "        for img_path in os.listdir(cls_path):\n",
    "            direct = os.path.join(cls_path,img_path)\n",
    "            df.append([direct,cls_name])\n",
    "    \n",
    "    df = pd.DataFrame(df, columns=['image','label'])\n",
    "    print(\"The number of samples found:\",len(df))\n",
    "    return df.copy()\n",
    "\n",
    "def read_image(path):\n",
    "    \"\"\"\n",
    "    Read an image from specified directory\n",
    "    _____________________________________________________________\n",
    "    Arguments:\n",
    "    \n",
    "    path: String -> a directory of the image\n",
    "    _____________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    image: numpy.array of the image\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def show_image(img, label=None) -> None:\n",
    "    \"\"\"\n",
    "    This function to display any image\n",
    "    _________________________________________________________\n",
    "    Arguements:\n",
    "    \n",
    "    img: numpy.array of N-D\n",
    "    \n",
    "    label: String -> the title/label added with the image, Default= None\n",
    "    _________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    plt.imshow()\n",
    "    \"\"\"\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "def clbck(model_name):\n",
    "    # The function is defined to make the callbacks for training the models\n",
    "    ERLY = EarlyStopping(patience=10, min_delta=0.01, start_from_epoch=10, verbose=1) # Stop training model if the performance does not improve\n",
    "    RD = ReduceLROnPlateau(patience=5, min_delta=0.01, factor=0.5) # Reduce the learning rate if the performance does not improve\n",
    "    CHK = ModelCheckpoint(f'{model_name}_model.keras',verbose=1, save_best_only=True) # Save the best performance model\n",
    "    return [ERLY,RD,CHK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b66b8c-1e7a-4ac4-967c-6fdb26a77360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined hyperparameters\n",
    "IMG_SHAPE = (32,32) # 32x32 pixels\n",
    "IMG_SIZE = (32,32,3) # 32x32 pixels and 3 RGB channel\n",
    "BATCH_SIZE = 32 # setting data size per 1 round (batch) before update weights\n",
    "opt = Adam(learning_rate=0.00001, epsilon=1e-6) # epsilon help avoiding zero division when calculate gradient\n",
    "loss = 'categorical_crossentropy'\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8b56e-f460-4d63-a26a-68405f34ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset in dataframe \n",
    "main_path = 'your_file/Dataset/OCR_dataset'\n",
    "df = directory_to_df(main_path)                   # convert the dataset into df of two columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ad398-016d-4cf3-8ed9-37b4837bf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7357d-85aa-4a7e-8892-200fd4a9861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training (70%) and testing (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['image'], df['label'], test_size=0.30, random_state=41)\n",
    "\n",
    "# Further splitting training set into training (75%) and validation (25%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=41)\n",
    "\n",
    "# Creating DataFrames\n",
    "training_df = pd.DataFrame({'image': X_train, 'label': y_train}) # 52.5% of overall data\n",
    "validation_df = pd.DataFrame({'image': X_valid, 'label': y_valid}) # 17.5% of overall data\n",
    "testing_df = pd.DataFrame({'image': X_test, 'label': y_test}) # 30% of overall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71c92a-c23a-45b7-8c0e-5f755e30e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating generators\n",
    "gen = ImageDataGenerator(dtype=np.int32, brightness_range=[0.0,1.0], fill_mode='nearest') # adjust brightness to do data augmentation\n",
    "gen2 = ImageDataGenerator(dtype=np.int32, fill_mode='nearest')\n",
    "train_gen = gen.flow_from_dataframe(training_df, x_col='image',y_col='label', batch_size=BATCH_SIZE, \n",
    "                                   target_size=IMG_SHAPE)\n",
    "valid_gen = gen2.flow_from_dataframe(validation_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                        target_size=IMG_SHAPE, shuffle=False)\n",
    "test_gen = gen2.flow_from_dataframe(testing_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                       target_size=IMG_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658e33c-8108-4cf9-9657-d16579577119",
   "metadata": {},
   "source": [
    "เนื่องจากโมเดล deep learning ไม่เข้าใจlabelของรูป(คลาส)จึงต้องแปลงเป็นIndex(ตัวเลข)ก่อน\n",
    "แต่เมื่อทำนายเสร็จ เราต้องแปลงกลับเป็นชื่อคลาสเพื่ออ่านผล เราจึงต้องเตรียมตัวแปร mapping และ mapping_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6c4cb-fdf0-4df0-afca-215e816beaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a mapping of the classes and the inverse for later processings\n",
    "mapping = train_gen.class_indices\n",
    "mapping_inverse = dict(map(lambda x: tuple(reversed(x)), mapping.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aebe21-6d2a-4fbf-af4c-2f346b57f170",
   "metadata": {},
   "source": [
    "ทดลองแสดงรูปภาพจากdataset โดยเลือกbatchที10จากtrain_genและเลือกรูปที่2ในbatchนั้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6700595-60f7-4743-b577-c9944d862e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a sample from the dataset\n",
    "BATCH_NUM = 10\n",
    "IMG_NUM = 2      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a38ca1-483e-4a0b-b5e1-2a6a76232bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading another sample from the dataset\n",
    "BATCH_NUM = 65\n",
    "IMG_NUM = 30      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267b19d-852d-432b-85ea-e9225e205a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(Input(shape=IMG_SIZE, batch_size=BATCH_SIZE, name='Input'))\n",
    "CNN_model.add(Conv2D(3, (3,3), strides=1, activation='relu', padding='same'))\n",
    "CNN_model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "CNN_model.add(MaxPool2D((3,3)))\n",
    "CNN_model.add(Conv2D(256, (3,3), activation='relu'))\n",
    "CNN_model.add(Dropout(0.2))\n",
    "CNN_model.add(Conv2D(256, (3,3), strides=2, activation='relu', padding='same'))\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "CNN_model.add(Dropout(0.2))\n",
    "CNN_model.add(Conv2D(1024, (2,2), activation='relu', padding='same'))\n",
    "CNN_model.add(MaxPool2D(2,2))\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(1024, activation='selu'))\n",
    "CNN_model.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f9c31-3c49-4da3-8d96-a2513faaf23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fd830-aca1-45c2-a719-db0c7222d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters of adam will be used for the custom CNN\n",
    "CNN_model.compile(optimizer=Adam(), loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec903b-6031-4604-b1f3-5248ca60cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different num. of epochs will be given for better convergence for the Custom CNN\n",
    "history = CNN_model.fit(train_gen, epochs=20, validation_data=valid_gen, callbacks=clbck(\"CustomCnn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e137d-a5b6-467d-839c-fe947e5bc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"Custom CNN Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111684e-35ed-4b8b-aaee-e7281696b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the Custom CNN for the testing set for the evaluation\n",
    "prediction = CNN_model.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0c540-ea98-4c44-95fc-0efea9c33ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe Custom CNN Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e9b9a-8805-49e3-a843-f184dc417dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB7 input layers will not be FREEZED\n",
    "train_layers = EfficientNetB7(include_top=False, input_shape=IMG_SIZE)\n",
    "EffNetB7 = Sequential()\n",
    "EffNetB7.add(train_layers)\n",
    "EffNetB7.add(Flatten())\n",
    "EffNetB7.add(Dense(1024, activation='selu'))\n",
    "EffNetB7.add(Dropout(0.2))\n",
    "EffNetB7.add(Dense(512, activation='selu'))\n",
    "EffNetB7.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbebc4-1b3c-451d-9c3e-08cd3654ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EffNetB7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e7e55-d906-4725-ad25-41c3510fde56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pre-defined optimizer will be used with too small learning rate\n",
    "EffNetB7.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3835e3-d206-4310-91ba-21e141c114e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = EffNetB7.fit(train_gen, epochs=EPOCHS, validation_data=valid_gen, callbacks=clbck(\"EfficientNetB7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10aba3-ddb1-4ca9-b362-571abe257926",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['loss'], label='Training loss')\n",
    "plt.plot(history2.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"EfficientNetB7 Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaed933-19c7-43bf-a1be-69c1c82a8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the EfficientNetB7 for the testing set for the evaluation\n",
    "prediction = EffNetB7.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f7724-2950-4756-9332-8ae517a5cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t     The EfficientNetB7 Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72cc5e-177a-4a69-8a6a-5d32aef42f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 input layers will not be FREEZED\n",
    "train_layers = MobileNetV2(include_top=False, input_shape=IMG_SIZE)\n",
    "MobNetV2 = Sequential()\n",
    "MobNetV2.add(train_layers)\n",
    "MobNetV2.add(Flatten())\n",
    "MobNetV2.add(Dense(1024, activation='selu'))\n",
    "MobNetV2.add(Dropout(0.2))\n",
    "MobNetV2.add(Dense(512, activation='selu'))\n",
    "MobNetV2.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99151e-ff7c-419a-acb3-e41b1c5d8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "MobNetV2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6603ee-4322-4f23-a9eb-87f36c7189f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_opt = Adam(learning_rate=0.00001, epsilon=1e-6)\n",
    "\n",
    "MobNetV2.compile(optimizer=new_opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba021cd1-4278-4edb-a81f-6e6cf802589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "history3 = MobNetV2.fit(train_gen, epochs=EPOCHS, validation_data=valid_gen, callbacks=clbck('MobileNetV2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dec7ad-a81e-4205-9192-ed1b277aa547",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history3.history['loss'], label='Training loss')\n",
    "plt.plot(history3.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"MobileNetV2 Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565dfb28-bc52-4fdb-af82-db095b3febdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the MobileNetV2 for the testing set for the evaluation\n",
    "prediction = MobNetV2.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a2613-2281-4f7f-8111-049a6b4a04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t     The MobileNetV2 Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d454c-8e5f-4a4f-aa18-9dcebca30a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 input layers will not be FREEZED\n",
    "train_layers = VGG19(include_top=False, input_shape=IMG_SIZE)\n",
    "VG = Sequential()\n",
    "VG.add(train_layers)\n",
    "VG.add(Flatten())\n",
    "VG.add(Dense(1024, activation='selu'))\n",
    "VG.add(Dense(512, activation='selu'))\n",
    "VG.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf3492-c607-4161-95fd-4473913b149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da938e3-6863-47fe-9f0b-39913681a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VG.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634b091-d1dd-4b3f-a0ce-b24c1fe95809",
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = VG.fit(train_gen, validation_data=valid_gen, epochs=EPOCHS, callbacks=clbck('VGG19'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91343ede-86c1-4d63-9009-3718b34099cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history4.history['loss'], label='Training loss')\n",
    "plt.plot(history4.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"VGG19 Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba220325-45b7-480a-a1e3-fcaa3fba37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the VGG19 for the testing set for the evaluation\n",
    "prediction = VG.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef6fe4-f20b-4094-8dc1-faf317833370",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe VGG19 Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f96de4-7681-4996-916c-38e3ab261b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNset121 input layers will not be FREEZED\n",
    "train_layers = DenseNet121(include_top=False, input_shape=IMG_SIZE)\n",
    "Den = Sequential()\n",
    "Den.add(train_layers)\n",
    "Den.add(Flatten())\n",
    "Den.add(Dense(1024, activation='selu'))\n",
    "Den.add(Dense(512, activation='selu'))\n",
    "Den.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aeb0fa-b71b-4285-ae71-ebf2d61e52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Den.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d419b-6adf-4ecc-941c-06e1e2237993",
   "metadata": {},
   "outputs": [],
   "source": [
    "Den.compile(optimizer=opt, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4df54-cc1c-4142-b2ed-50a63ac17b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history5 = Den.fit(train_gen, validation_data=valid_gen, epochs=20, callbacks=clbck(\"DenseNet121\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a24d9-f275-453f-986e-a43c0f277130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history5.history['loss'], label='Training loss')\n",
    "plt.plot(history5.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"DenseNet121 Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da0998-368b-419a-a7fb-d358770434ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the VGG19 for the testing set for the evaluation\n",
    "prediction = Den.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92b298-f4b1-47d8-8a28-89b02184e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe DenseNet121 Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9f80a-224b-4419-9159-0f5c998e26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = {'accuracy':[0.95,0.93,0.71,0.96,0.95], 'precision':[0.96,0.94,0.72,0.96,0.95],\n",
    "       'recall':[0.95,0.93,0.71,0.96,0.95], 'F1-Score':[0.95,0.93,0.71,0.96,0.95]}\n",
    "results = pd.DataFrame(vals, index=['Custom CNN','EfficientNetB7','MobileNetV7','VGG19',\n",
    "                                   'DenseNet121'])\n",
    "\n",
    "print(\"\\t\\tThe Evaluation results of CNN/Pre-trained models\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fd357-e013-44ad-94b5-9b5fd286ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=results, x=results.index, y='accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Models')\n",
    "plt.title(\"Models Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6531c-60c1-446f-9bf7-4fe5aa3e3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=results, x=results.index, y='precision')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Models')\n",
    "plt.title(\"Models Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ad48f-5671-45ab-986d-c2410e659d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=results, x=results.index, y='recall')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Models')\n",
    "plt.title(\"Models Recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd882611-113a-4a9f-8605-5190ddae22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=results, x=results.index, y='F1-Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Models')\n",
    "plt.title(\"Models F1-Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e21c5-3ccb-4053-85d7-4e3eba2ea2f5",
   "metadata": {},
   "source": [
    "Post-Processing\n",
    "ในส่วนนี้จะเป็นรายะเอียดการใช้ LOW LEVEL COMPUTER VISION TECHNIQUES เพื่อปรับปรุงinputให้ดีขึ้นไม่ว่าจะเป็นข้อความหรือลายมือ\n",
    "1. binarization ช่วยแปลงภาพสีเทาเป็นภาพขาวดำโดยเลือกค่าthresholdที่เหมาะสมอัตโนมัติ เหมาะกับOCR (อ่านตัวอักษรจากภาพ) และ การตรวจจับขอบภาพ (Edge Detection)\n",
    "2. Dilate ช่วยขยายวัตถุสีขาวในภาพไบนารีใช้ในงาน Computer Vision เพื่อให้ภาพสะอาดขึ้น และให้วัตถุชัดเจนขึ้น โดยการขยายวัตถุสีขาวในภาพไบนารี\n",
    "3. Find Rectangles ใช้ค้นหาและจัดเรียง Bounding Box ของวัตถุในภาพไบนารี ช่วยให้สามารถ แยกตัวอักษร, ตรวจจับตำแหน่งวัตถุ, และ Track Object ได้ง่ายขึ้น\n",
    "4. Extractนำ3ขั้นตอนก่อนหน้ามาร้อยเข้าด้วยกันเพื่อแสดงผลลัพธ์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e71a5-3089-4593-8b7b-449641e07f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision - Low level techniques\n",
    "def load_model():\n",
    "    model_path = '/your_directory/working/DenseNet121_model.h5'\n",
    "    model = tf.keras.saving.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def convert_2_gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_image\n",
    "\n",
    "def binarization(image):\n",
    "    img, thresh = cv2.threshold(image, 0,255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "    return img, thresh\n",
    "\n",
    "def dilate(image, words= False):\n",
    "    img = image.copy()\n",
    "    m = 3                       # m = defalut kernel size in horizontal, n = defalut kernel size in vertical\n",
    "    n = m - 2                   # n less than m for Vertical structuring element to dilate chars\n",
    "    itrs = 4                    # iteration for dilation\n",
    "    if words:\n",
    "        m = 6                   # increase kernel size in horizontal\n",
    "        n = m                   # increase kernel size in vertical equal to m\n",
    "        itrs = 3                # set 3 iteration for words\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (n, m))\n",
    "    dilation = cv2.dilate(img, rect_kernel, iterations = itrs)\n",
    "    return dilation\n",
    "\n",
    "def find_rect(image):\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    rects = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)  # Extract the bounding rectangle coordinates of each countour\n",
    "        rects.append([x,y,w,h])\n",
    "        \n",
    "    sorted_rects = list(sorted(rects, key=lambda x: x[0])) # Sorting the rects from Left-to-Right\n",
    "    return sorted_rects\n",
    "\n",
    "def extract(image):\n",
    "    model = load_model()\n",
    "    chars = []              # a list to store recognized characters\n",
    "    \n",
    "    image_cpy = image.copy()\n",
    "    _, bin_img = binarization(convert_2_gray(image_cpy))\n",
    "    full_dil_img = dilate(bin_img,words=True)\n",
    "    words = find_rect(full_dil_img)                       # Recognized words within the image \n",
    "    del _, bin_img, full_dil_img                          # for better memory usage\n",
    "    \n",
    "    for word in words:\n",
    "        x,y,w,h = word                                    # coordinates of the word\n",
    "        img = image_cpy[y:y+h, x:x+w]\n",
    "        \n",
    "        _, bin_img = binarization(convert_2_gray(img))\n",
    "        dil_img = dilate(bin_img)\n",
    "        char_parts = find_rect(dil_img)                     # Recognized chars withtin the word\n",
    "        cv2.rectangle(image, (x,y),(x+w,y+h), (0,255,0), 3) # draw a green rectangle around the word\n",
    "        \n",
    "        del _, bin_img, dil_img\n",
    "        \n",
    "        for char in char_parts:    \n",
    "            x,y,w,h = char\n",
    "            ch = img[y:y+h, x:x+w]\n",
    "            \n",
    "            empty_img = np.full((32,32,1),255, dtype=np.uint8) # a white image used for resize with filling\n",
    "            x,y = 3,3                                          # starting indecies\n",
    "            resized = cv2.resize(ch, (16,22), interpolation=cv2.INTER_CUBIC)\n",
    "            gray = convert_2_gray(resized)\n",
    "            empty_img[y:y+22, x:x+16,0] = gray.copy()          # integrate the recognized char into the white image\n",
    "            gray = cv2.cvtColor(empty_img, cv2.COLOR_GRAY2RGB)\n",
    "            gray = gray.astype(np.int32)\n",
    "            \n",
    "            predicted = mapping_inverse[np.argmax(model.predict(np.array([gray]), verbose=-1))]\n",
    "            chars.append(predicted)                            # append the character into the list\n",
    "            \n",
    "            del ch, resized, gray, empty_img\n",
    "        chars.append(' ')  # at the end of each iteration (end of word) append a space\n",
    "        \n",
    "    del model\n",
    "    show_image(image)\n",
    "    return ''.join(chars[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757c904-60ad-4e6a-9eaf-9710f30573ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 1\n",
    "img = read_image('/your_directory/myimage16/Screenshot 2023-12-19 024132.png')\n",
    "text = extract(img)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae7aae-6592-400f-a938-e10b0fe8e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 2\n",
    "img2 = read_image('/your_directory/myimage15/Screenshot 2023-12-19 023610.png')\n",
    "text = extract(img2)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274376a1-9326-41c6-9505-0a4cda6b84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 3\n",
    "img3 = read_image('/your_directory/myimage12/Screenshot 2023-12-19 021326.png')\n",
    "text = extract(img3)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da685e7c-7748-4985-97df-7560573d36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 4\n",
    "img4 = read_image('/your_directory/myimage8/Screenshot 2023-12-19 011000.png')\n",
    "text = extract(img4)\n",
    "print('-->',text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
